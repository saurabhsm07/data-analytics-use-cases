{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Classificaion:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#required libraries:\n",
    "import pandas as pd;    \n",
    "import numpy as np;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing training and test data into pandas dataframe\n",
    "def import_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#utility functions\n",
    "def print_info(df):\n",
    "    return df.info()\n",
    "\n",
    "def print_description(df):\n",
    "    return df.describe()\n",
    "\n",
    "def print_head(df, count=5):\n",
    "    return df.head(count)\n",
    "\n",
    "# function returns a dictionary of percentage of null values in a dataframe\n",
    "def get_null_percent(df):\n",
    "    null_percent_dict = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        null_percent_dict[col] = df[df[col].isnull() == True].loc[:,'Survived'].count()/ df.shape[0] *100\n",
    "    \n",
    "    return null_percent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get list of attributes with atleast 1 cell having a null value\n",
    "def get_attributes_with_nulls(df):\n",
    "    df_nulls = pd.Series(df.isna().any())\n",
    "    return df_nulls[df_nulls == True].index.tolist()\n",
    "\n",
    "def get_attributes_with_nulls_inorder(df):\n",
    "    return    (df.isnull()\n",
    "                 .sum(axis = 0)\n",
    "                 .loc[df.isnull().sum() > 0]\n",
    "                 .sort_values( ascending = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out results of 3 different stages of logistic regression\n",
    "def print_logistic_regression_results(df):\n",
    "    print(\" simple logistic regg = {} \\n min-max scaled logistic regg = {} \\n grid -searched logistic regg = {}\".\n",
    "    format(logistic_regression_model(df), scaled_logistic_regression(df), gridSearch_logistic_regression_model(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess both training and testing dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def numeric_mapper(df, column_name):\n",
    "    mapper = {}\n",
    "    data_list = df[column_name].unique()\n",
    "    data_list = ['missing' if pd.isnull(x) else x for x in data_list]\n",
    "    data_list.sort()\n",
    "    for i in range(0, len(data_list)):\n",
    "        if data_list[i] == 'missing':\n",
    "            mapper[np.nan] = 404\n",
    "        else:    \n",
    "            mapper[data_list[i]] = i\n",
    "    return mapper\n",
    "    \n",
    "def data_preprocessor(df):\n",
    " \n",
    "    df = (df.rename({'SibSp' : '# of Siblings', \n",
    "                                             'Parch': '# of Parents', \n",
    "                                             'Sex' : 'Gender',\n",
    "                                             'Pclass' : 'Class'},\n",
    "                                            axis = 1)\n",
    "                                    .drop(['Name', 'Ticket', 'PassengerId'], axis = 1)\n",
    "                                    .astype({'Gender' : pd.api.types.CategoricalDtype(df['Sex'].unique(), ordered=False), \n",
    "                                             'Class' : pd.api.types.CategoricalDtype(df['Pclass'].unique(), ordered=True)})\n",
    "#                                     .replace({'Embarked' : {np.NaN : 'un-known'}})\n",
    "                                    .replace({'Gender' : numeric_mapper(df, 'Sex'),\n",
    "                                              'Embarked' : numeric_mapper(df, 'Embarked'),\n",
    "                                              'Cabin' : numeric_mapper(df, 'Cabin'),\n",
    "                                              'Age' : {np.nan : 404}})\n",
    "                                    \n",
    "    #                                 .loc[:]\n",
    "                      )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing With Null Values In Attributes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASE 1 : Removing attributes with null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In training only 2 attributes contains a major chunk of null data : Age and Cabin\n",
    "def drop_null_attributes(df):\n",
    "    df = (df.dropna(axis='columns')\n",
    "         )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASE 2: Remove rows with null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def drop_null_rows(df):\n",
    "    df = (df.dropna(axis='rows')\n",
    "         )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASE 3: Replacing null attribute values with mean, median or mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replace nulls with averages \n",
    "def replace_null_with_mean(col):\n",
    "    if col.dtype.name == \"category\":\n",
    "        col = col.replace(np.nan,col.mode())\n",
    "    else:\n",
    "        col = col.replace(np.nan, col.mean())\n",
    "    return col\n",
    "\n",
    "def replace_nulls_phase_1(df):\n",
    "    df = (df.apply(replace_null_with_mean, axis = 0)\n",
    "         )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASE 4: Group attributes with similar values and replace null values with mean or mode values of that specific group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to return attributes with greatest correlation with the provided attribute\n",
    "def top_correlations(df,attribute, count = 2):\n",
    "    correlations_df  = df.corr()\n",
    "    correlation_attribute = correlations_df[attribute]\n",
    "    correlation_attribute = correlation_attribute.to_frame()\n",
    "    correlation_attribute[attribute+'_mod'] = [ x if x > 0 else -1*x for x in correlations_df[attribute]]\n",
    "    return (correlation_attribute.sort_values(attribute+'_mod', ascending = False)[1: (count+1)].loc[:, attribute])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# form a dictionary structure of column with nulls as keys and attributes they are most correlated to as a list of values\n",
    "def get_top_corr_dict(attribute_with_nulls):\n",
    "    top_corr_dict = {}\n",
    "    for attribute in attribute_with_nulls:\n",
    "        top_corr_dict[attribute] = top_correlations(titanic_train_4, attribute)\n",
    "    return top_corr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataframe of values based on averages columns with null values based on groupby result of previous step\n",
    "def get_vals_based_on_corr_groups(top_corr_dict, attr_type):\n",
    "    values_based_corr_dict = {}\n",
    "    for attr in top_corr_dict.keys():\n",
    "        if attr_type[attr] == 'category':\n",
    "            df = titanic_train_4.groupby(top_corr_dict[attr].index.tolist(), as_index= False)[attr].agg(lambda x : x.value_counts().index[0] if len(x.value_counts()) is not 0 else titanic_train_4[attr].value_counts().index[1])\n",
    "        else:\n",
    "            df = titanic_train_4.groupby(top_corr_dict[attr].index.tolist(),  as_index= False)[attr].agg(lambda x : x.mean() if pd.isnull(x.mean()) is not True else titanic_train_4[attr].value_counts().index[0])\n",
    "        values_based_corr_dict[attr] = df \n",
    "    return values_based_corr_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASE 5 A: replace null values by creating a simple prediction model from remaining non null attributes in  (PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [x for x in titanic_train_5.columns if x not in [y for y in attribute_with_nulls if y != 'Age']] - list comprehension to remove non important nulls\n",
    "\n",
    "# split groups into train and predict dataframe sets based on condition if attribute.val = np.nan\n",
    "def attr_train_predict_split(df, attribute_with_nulls, attr):\n",
    "    df= df.loc[:, [x for x in df.columns if x not in [y for y in attribute_with_nulls if y != attr]]]\n",
    "    train_set = df[np.isnan(df[attr]) == False]\n",
    "    predict_set = df[np.isnan(df[attr])]\n",
    "    return (train_set, predict_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict substitutes for null values by training a simple KNN model on the data split of non null attribute values\n",
    "def predict_nulls(train_set, predict_set, attr, attr_type):\n",
    "    from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "    X_train = train_set.loc[:, train_set.columns != attr]\n",
    "    y_train = train_set[attr]\n",
    "\n",
    "    X_predict = predict_set.loc[:, predict_set.columns != attr]\n",
    "    y_predict = predict_set[attr]\n",
    "    \n",
    "    if(attr_type == 'category'):\n",
    "        return KNeighborsClassifier().fit(X_train, y_train).predict(X_predict)\n",
    "    else:\n",
    "        return KNeighborsRegressor().fit(X_train, y_train).predict(X_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASE 5 B: replace null values by creating a simpe prediction model from remaining non null attributes in (SERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split groups into train and predict dataframe sets based on condition if attribute.val = np.nan\n",
    "def attr_train_predict_split_inorder(df, attribute_with_nulls_inorder, attr):\n",
    "    df= df.loc[:, [x for x in df.columns if (df[x].isna().any() != True) | (x == attr) ]]\n",
    "    train_set = df[np.isnan(df[attr]) == False]\n",
    "    predict_set = df[np.isnan(df[attr])]\n",
    "    return (train_set, predict_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict substitutes for null values by training a simple KNN model on the data split of non null attribute values\n",
    "def predict_nulls_inorder(train_set, predict_set, attr, attr_type):\n",
    "    from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "    X_train = train_set.loc[:, train_set.columns != attr]\n",
    "    y_train = train_set[attr]\n",
    "\n",
    "    X_predict = predict_set.loc[:, predict_set.columns != attr]\n",
    "    y_predict = predict_set[attr]\n",
    "    \n",
    "    if(attr_type == 'category'):\n",
    "        return KNeighborsClassifier().fit(X_train, y_train).predict(X_predict)\n",
    "    else:\n",
    "        return KNeighborsRegressor().fit(X_train, y_train).predict(X_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale_data(train_set, test_set):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range= (-1,1))\n",
    "    train_scaled = scaler.fit_transform(train_set)\n",
    "    test_scaled = scaler.transform(test_set)\n",
    "    return (train_scaled, test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler: <font color='red'> in progress</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled data to a mean = 0 , std = 1\n",
    "def standard_scale_data(train_set, test_set):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    std_scaler = StandardScaler()\n",
    "    std_scaler.fit(train_set)\n",
    "    train_scaled = std_scaler.transform(train_set)\n",
    "    test_scaled = std_scaler.transform(test_set)\n",
    "    return (train_scaled, test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA (Principal Component Analysis) : <font color='red'> work in progress</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_dataset(train_set, test_set, features = 2):\n",
    "    from sklearn.decomposition import PCA\n",
    "    train_scaled, test_scaled = standard_scale_data(train_set, test_set)\n",
    "    pca = PCA(n_components= features)\n",
    "    pca.fit(train_scaled)\n",
    "    pca_train = pca.transform(train_scaled)\n",
    "    pca_test = pca.transform(test_scaled)\n",
    "    return (pca_train, pca_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Splitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_data(df):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns!= 'Survived'], df['Survived'], test_size = 0.75, random_state = 0)\n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap : correlation matrix visualization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a heatmap of correlation matrix of attribute variables\n",
    "def plot_correlation_heatmap(correlation_data):\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    import seaborn as sns\n",
    "    return sns.heatmap(correlation_data, annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision boundary view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic_regression():\n",
    "    X_mat = X\n",
    "    y_mat = y\n",
    "\n",
    "    # Create color maps\n",
    "    cmap_light = ListedColormap(['#FFFFAA', '#AAFFAA', '#AAAAFF','#EFEFEF'])\n",
    "    cmap_bold  = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X_mat, y_mat)\n",
    "\n",
    "    # Plot the decision boundary by assigning a color in the color map\n",
    "    # to each mesh point.\n",
    "    \n",
    "    mesh_step_size = .01  # step size in the mesh\n",
    "    plot_symbol_size = 50\n",
    "    \n",
    "    x_min, x_max = X_mat[:, 0].min() - 1, X_mat[:, 0].max() + 1\n",
    "    y_min, y_max = X_mat[:, 1].min() - 1, X_mat[:, 1].max() + 1\n",
    "    xx, yy = numpy.meshgrid(numpy.arange(x_min, x_max, mesh_step_size),\n",
    "                         numpy.arange(y_min, y_max, mesh_step_size))\n",
    "    Z = clf.predict(numpy.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot training points\n",
    "    plt.scatter(X_mat[:, 0], X_mat[:, 1], s=plot_symbol_size, c=y, cmap=cmap_bold, edgecolor = 'black')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    title = \"Neighbors = {}\".format(n_neighbors)\n",
    "    if (X_test is not None):\n",
    "        train_score = clf.score(X_mat, y_mat)\n",
    "        test_score  = clf.score(X_test, y_test)\n",
    "        title = title + \"\\nTrain score = {:.2f}, Test score = {:.2f}\".format(train_score, test_score)\n",
    "\n",
    "    patch0 = mpatches.Patch(color='#FFFF00', label='class 0')\n",
    "    patch1 = mpatches.Patch(color='#000000', label='class 1')\n",
    "    plt.legend(handles=[patch0, patch1])\n",
    "\n",
    "    plt.xlabel('Feature 0')\n",
    "    plt.ylabel('Feature 1')\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_model(df):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns!= 'Survived'], df['Survived'], test_size = 0.75, random_state = 0)\n",
    "    clf = LogisticRegression(solver = 'liblinear') #solver : liblinear good for small datasets\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Searched : Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  gridSearch_logistic_regression_model(df):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    logistic_reg = LogisticRegression(solver = 'liblinear')\n",
    "    parameters = {'penalty' : ('l1', 'l2'),\n",
    "                   'C' : [0.1, 10]\n",
    "                 }\n",
    "    clf = GridSearchCV(logistic_reg, parameters, cv = 5, iid= True)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns!= 'Survived'], df['Survived'], test_size = 0.75, random_state = 0)\n",
    "    X_train, X_test = min_max_scale_data(X_train, X_test)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.score(X_test, y_test)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_logistic_regression(df):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    clf = LogisticRegression(solver = 'liblinear')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns!= 'Survived'], df['Survived'], test_size = 0.75, random_state = 0)\n",
    "    X_train, X_test = min_max_scale_data(X_train, X_test)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.score(X_test, y_test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic_train = import_data(\"./../../../Datasets/Titanic/train.csv\");\n",
    "titanic_test =  import_data(\"./../../../Datasets/Titanic/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocessing phase 1 for training and validation dataset\n",
    "titanic_train = data_preprocessor(titanic_train)\n",
    "print_head(titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic_test = data_preprocessor(titanic_test)\n",
    "print_head(titanic_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Case 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removed columns with null values\n",
    "titanic_train_1 = titanic_train.replace({404 : np.nan})\n",
    "titanic_train_1 = drop_null_attributes(titanic_train_1)\n",
    "logistic_regression_model(titanic_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Case 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# removed rows with null values\n",
    "titanic_train_2 = titanic_train.replace({404 : np.nan})\n",
    "titanic_train_2 = drop_null_rows(titanic_train_2)\n",
    "logistic_regression_model(titanic_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Case 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replaced null values with mean and mode\n",
    "titanic_train_3 = titanic_train.replace({404 : np.nan})\n",
    "titanic_train_3 = replace_nulls_phase_1(titanic_train_3)\n",
    "logistic_regression_model(titanic_train_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Case 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replace nulls with correlation and groupby based values\n",
    "titanic_train_4 = (titanic_train.replace({404 : np.nan})\n",
    "                                .astype({'Class' : int})\n",
    "                  )\n",
    "\n",
    "attribute_with_nulls = get_attributes_with_nulls(titanic_train_4)\n",
    "top_corr_dict = get_top_corr_dict(attribute_with_nulls)\n",
    "\n",
    "# used to switch mechanism of finding null value replacements based on type of attribute\n",
    "attr_type = {'Age' : 'discrete', 'Cabin' : 'category', 'Embarked': 'category'}\n",
    "vals_based_on_corr_groups = get_vals_based_on_corr_groups(top_corr_dict, attr_type)\n",
    "\n",
    "# function to replace null values in the dataframe using dictionary of null replacments gained using previous steps data\n",
    "def update_null_vals(row, vals_based_on_corr_groups):\n",
    "    for attr in vals_based_on_corr_groups.keys():\n",
    "        if np.isnan(row[attr]):\n",
    "            frame = vals_based_on_corr_groups[attr]\n",
    "            row[attr] = frame[(frame[frame.columns[0]] == row[frame.columns[0]]) & (frame[frame.columns[1]] == row[frame.columns[1]]) ].loc[:, attr]\n",
    "    \n",
    "    return row\n",
    "\n",
    "titanic_train_4 = titanic_train_4.apply(update_null_vals, axis = 1, args= (vals_based_on_corr_groups, ));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logistic_regression_model(titanic_train_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Case 5 A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic_train_5_A = (titanic_train.replace({404 : np.nan})\n",
    "                                .astype({'Class' : int})\n",
    "                  )\n",
    "\n",
    "# get list of attributes which have at least 1 null value in the cells\n",
    "attribute_with_nulls = get_attributes_with_nulls(titanic_train_5_A)\n",
    "\n",
    "# used to switch between regression and classification model  of finding null value replacements based on type of attribute\n",
    "attr_type = {'Age' : 'discrete', 'Cabin' : 'category', 'Embarked': 'category'}\n",
    "\n",
    "# loop through and predict null attribute values by splitting data using function 1 and predicting values using function 2\n",
    "for attr in attribute_with_nulls:\n",
    "    #function 1\n",
    "    train, predict = attr_train_predict_split(titanic_train_5_A, attribute_with_nulls, attr) \n",
    "    #function 2\n",
    "    predicted_values = predict_nulls(train, predict, attr, attr_type[attr]) \n",
    "    # replace nulls with prediction reuslts\n",
    "    titanic_train_5_A.loc[np.isnan(titanic_train_5_A[attr]), attr] =  predicted_values \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logistic_regression_model(titanic_train_5_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### TEST Case 5 B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train_5_B= (titanic_train.replace({404 : np.nan})\n",
    "                                .astype({'Class' : int})\n",
    "                  )\n",
    "\n",
    "# get list of attributes which have at least 1 null value in the cells in decending order with count\n",
    "attribute_with_nulls_inorder = get_attributes_with_nulls_inorder(titanic_train_5_B)\n",
    "\n",
    "# used to switch between regression and classification model  of finding null value replacements based on type of attribute\n",
    "attr_type = {'Age' : 'discrete', 'Cabin' : 'category', 'Embarked': 'category'}\n",
    "\n",
    "#predicting the attribute with least nulls and using them to predict future null attribute values iteratively\n",
    "for attr in attribute_with_nulls_inorder.index.tolist():\n",
    "    train, predict = attr_train_predict_split_inorder(titanic_train_5_B, attribute_with_nulls_inorder.index.tolist(), attr)\n",
    "    predicted_values = predict_nulls(train, predict, attr, attr_type[attr]) \n",
    "    titanic_train_5_B.loc[np.isnan(titanic_train_5_B[attr]), attr] =  predicted_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model(titanic_train_5_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch_logistic_regression_model(titanic_train_5_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_logistic_regression(titanic_train_5_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train = (titanic_train.replace({404 : np.nan})\n",
    "                                .astype({'Class' : int})\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_null_percent(titanic_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Null Value Percentages :__\n",
    "- <font color='red'> Cabin : 77.01% </font>\n",
    "- <font color='blue'> Age : 19.86% </font>\n",
    "- <font color='green'> Embarked : 0.22% </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(titanic_train.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = titanic_train.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "table {\n",
    "  font-family: arial, sans-serif;\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;\n",
    "}\n",
    "\n",
    "td, th {\n",
    "  border: 1px solid #dddddd;\n",
    "  text-align: left;\n",
    "  padding: 8px;\n",
    "}\n",
    "\n",
    "tr:nth-child(even) {\n",
    "  background-color: #dddddd;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h4>Correlation analysis table for attribute with null values:</h4>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Attribute</th>\n",
    "    <th>Correlation with classification attribute (Survived)</th>\n",
    "    <th>Max Correlation attribute 1</th>\n",
    "    <th>Max Correlation attribute 2</th>  \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Embarked</td>\n",
    "    <td>-0.169718</td>\n",
    "    <td>Fare (- 0.2263)</td>\n",
    "    <td>Class (0.164681)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Age</td>\n",
    "    <td>-0.077221</td>\n",
    "    <td>Class (-0.369226)</td>\n",
    "    <td> # of Siblings (-0.308247)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Cabin</td>\n",
    "    <td>0.029619</td>\n",
    "    <td>Class (0.493209) </td> \n",
    "    <td>Fare (-0.259622) </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iteration 1:\n",
    "**based on the above analysis the null/empty columns can be indivisually handled as follows:**\n",
    "<br>\n",
    "\n",
    "**Embarked**\n",
    "- Strategy 1: remove rows where 'Embarked' attribute is null\n",
    "- Strategy 2: replace with majory class\n",
    "<br>\n",
    "\n",
    "**Age**\n",
    "- Strategy 1: replace with group based majority based on correlation attributes\n",
    "- Strategy 2: replace with prediction model results based on correlation attributes\n",
    "- Strategy 3: replace with mean or median of the column\n",
    "<br>\n",
    "\n",
    "**Cabin**\n",
    "- Strategy 1: remove column from analysis dataset\n",
    "- Strategy 2: replace with prediction model results based on correlation attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1:\n",
    "# removing rows having 'Embarked'as null\n",
    "# removing attribute 'Cabin'\n",
    "\n",
    "train_data = (titanic_train.dropna(axis = 'rows', subset=['Embarked'])\n",
    "              .drop('Cabin', axis=1)\n",
    ")\n",
    "\n",
    "#step 2:\n",
    "# replace 'Age' with mean value of the column based on correlation attributes\n",
    "age_mean_corr = titanic_train.groupby(['Class', '# of Siblings'], as_index= False)['Age'].agg(lambda x : x.mean() if pd.isnull(x.mean()) is not True else titanic_train['Age'].mean())\n",
    "\n",
    "def replace_null_corr(row, age_mean_corr):\n",
    "    if np.isnan(row['Age']):\n",
    "        row['Age'] = age_mean_corr.loc[(age_mean_corr['Class'] == row['Class']) & (age_mean_corr['# of Siblings'] == row['# of Siblings']),'Age']\n",
    "    return row\n",
    "train_data = train_data.apply(replace_null_corr, axis = 1, args=(age_mean_corr, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_logistic_regression_results(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train_pca = (titanic_train.replace({np.nan : 404})\n",
    "                  )\n",
    "\n",
    "train_set, test_set, y_train, y_test = train_test_split_data(titanic_train_pca)\n",
    "train_pca, test_pca = get_pca_dataset(train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> important information below :</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Scope:\n",
    "\n",
    "- automate preprocessing end to end for all features\n",
    "- functional groupby for discrete values like \"Fare\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
